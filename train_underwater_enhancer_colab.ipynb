{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Underwater Image Enhancement with Autoencoder\nThis notebook trains an autoencoder model to enhance underwater GoPro images, matching manually edited quality.\n\n## üöÄ Quick Start\n1. **Run in Google Colab** with GPU runtime (Runtime ‚Üí Change runtime type ‚Üí GPU)\n2. **Mount Google Drive** (dataset should be pre-uploaded)\n3. **Train the model** using your pre-processed dataset\n\n## üìÅ Expected Dataset Structure\nYour dataset should be in Google Drive at:\n```\n/content/drive/MyDrive/testing-dataset-1000-underwater/\n‚îú‚îÄ‚îÄ input/          # Input TIFF images\n‚îú‚îÄ‚îÄ target/         # Target (manually edited) TIFF images\n‚îî‚îÄ‚îÄ split.txt       # Train/validation split indices\n```"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f'GPU Available: {torch.cuda.get_device_name(0)}')\n",
    "    print(f'Memory Available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print('No GPU available, using CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install required packages\n!pip install -q pillow torch torchvision tqdm matplotlib tensorboard"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nfrom torchvision.utils import save_image\nfrom tqdm.auto import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Set random seeds for reproducibility\ntorch.manual_seed(42)\nnp.random.seed(42)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. Mount Google Drive and Setup Paths"
  },
  {
   "cell_type": "markdown",
   "source": "## 2a. Mount Google Drive and Load Dataset",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Mount Google Drive\nfrom google.colab import drive\nimport os\nfrom pathlib import Path\n\ndrive.mount('/content/drive')\n\n# Dataset path (pre-uploaded)\nDATASET_PATH = Path('/content/drive/MyDrive/testing-dataset-1000-underwater')\nINPUT_DIR = DATASET_PATH / 'input'\nTARGET_DIR = DATASET_PATH / 'target'\nSPLIT_FILE = DATASET_PATH / 'split.txt'\n\n# Output directories\nOUTPUT_BASE = Path('/content/drive/MyDrive/underwater_enhancement')\nMODELS_PATH = OUTPUT_BASE / 'models'\nCHECKPOINT_PATH = OUTPUT_BASE / 'checkpoints'\n\n# Create output directories\nOUTPUT_BASE.mkdir(exist_ok=True)\nMODELS_PATH.mkdir(exist_ok=True)\nCHECKPOINT_PATH.mkdir(exist_ok=True)\n\n# Verify dataset exists\nif not DATASET_PATH.exists():\n    print(f\"‚ùå Dataset not found at {DATASET_PATH}\")\n    print(\"Please upload your dataset to Google Drive first.\")\nelse:\n    print(f\"‚úì Dataset found at: {DATASET_PATH}\")\n    print(f\"‚úì Input images: {INPUT_DIR}\")\n    print(f\"‚úì Target images: {TARGET_DIR}\")\n    print(f\"‚úì Models will be saved at: {MODELS_PATH}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load image filenames and split information\nimport json\n\n# Get list of all images - check for both .tiff and .tif extensions\ninput_files = sorted([f for f in os.listdir(INPUT_DIR) if f.endswith(('.tiff', '.tif', '.TIFF', '.TIF'))])\ntarget_files = sorted([f for f in os.listdir(TARGET_DIR) if f.endswith(('.tiff', '.tif', '.TIFF', '.TIF'))])\n\nprint(f\"Found {len(input_files)} input images\")\nprint(f\"Found {len(target_files)} target images\")\n\n# If no target files found with TIFF extensions, check for other image formats\nif len(target_files) == 0:\n    target_files = sorted([f for f in os.listdir(TARGET_DIR) if f.endswith(('.png', '.PNG', '.jpg', '.JPG', '.jpeg', '.JPEG'))])\n    print(f\"Found {len(target_files)} target images with alternative extensions\")\n\n# Load train/validation split\nif SPLIT_FILE.exists():\n    try:\n        # Try to load as JSON first\n        with open(SPLIT_FILE, 'r') as f:\n            split_data = json.load(f)\n        train_indices = split_data['train']\n        val_indices = split_data['validation']\n    except json.JSONDecodeError:\n        # If not JSON, parse as plain text with comma-separated indices\n        with open(SPLIT_FILE, 'r') as f:\n            lines = f.readlines()\n        \n        train_indices = []\n        val_indices = []\n        \n        for line in lines:\n            line = line.strip()\n            if 'Training' in line or 'training' in line:\n                continue\n            elif 'Validation' in line or 'validation' in line:\n                continue\n            elif line and not line.startswith('#'):\n                # Parse comma-separated indices\n                indices = [int(x.strip()) for x in line.split(',') if x.strip().isdigit()]\n                \n                # Determine if these are train or validation indices based on position\n                if not train_indices:  # First set of indices is training\n                    train_indices.extend(indices)\n                else:  # Second set is validation\n                    val_indices.extend(indices)\n        \n    print(f\"‚úì Loaded split: {len(train_indices)} train, {len(val_indices)} validation\")\nelse:\n    # If no split file, create 80/20 split\n    print(\"No split.txt found, creating 80/20 train/validation split\")\n    n_images = min(len(input_files), len(target_files))\n    indices = np.random.permutation(n_images)\n    split_point = int(0.8 * n_images)\n    train_indices = indices[:split_point].tolist()\n    val_indices = indices[split_point:].tolist()\n    print(f\"Created split: {len(train_indices)} train, {len(val_indices)} validation\")\n\n# Verify we have matching input and target files\nif len(input_files) != len(target_files):\n    print(f\"‚ö†Ô∏è Warning: Number of input files ({len(input_files)}) doesn't match target files ({len(target_files)})\")\n    print(\"Using the minimum number of files available\")\n    min_files = min(len(input_files), len(target_files))\n    input_files = input_files[:min_files]\n    target_files = target_files[:min_files]"
  },
  {
   "cell_type": "code",
   "source": "# Display a sample image pair\nsample_idx = 0\ninput_path = INPUT_DIR / input_files[sample_idx]\ntarget_path = TARGET_DIR / target_files[sample_idx]\n\nfig, axes = plt.subplots(1, 2, figsize=(12, 6))\n\ninput_img = Image.open(input_path)\ntarget_img = Image.open(target_path)\n\naxes[0].imshow(input_img)\naxes[0].set_title(f'Input Image\\n{input_files[sample_idx]}')\naxes[0].axis('off')\n\naxes[1].imshow(target_img)\naxes[1].set_title(f'Target Image\\n{target_files[sample_idx]}')\naxes[1].axis('off')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Input image size: {input_img.size}\")\nprint(f\"Target image size: {target_img.size}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "class UnderwaterDataset(Dataset):\n    def __init__(self, input_dir, target_dir, file_indices, image_size=512, augment=False):\n        self.input_dir = Path(input_dir)\n        self.target_dir = Path(target_dir)\n        self.augment = augment\n        \n        # Get all files with various image extensions\n        input_extensions = ('.tiff', '.tif', '.TIFF', '.TIF')\n        target_extensions = ('.tiff', '.tif', '.TIFF', '.TIF', '.png', '.PNG', '.jpg', '.JPG', '.jpeg', '.JPEG')\n        \n        all_input_files = sorted([f for f in os.listdir(input_dir) if f.endswith(input_extensions)])\n        all_target_files = sorted([f for f in os.listdir(target_dir) if f.endswith(target_extensions)])\n        \n        # Filter indices to valid range\n        valid_indices = [i for i in file_indices if i < min(len(all_input_files), len(all_target_files))]\n        \n        # Select files based on indices\n        self.input_files = [all_input_files[i] for i in valid_indices]\n        self.target_files = [all_target_files[i] for i in valid_indices]\n        \n        # Define transforms\n        transform_list = [\n            transforms.Resize((image_size, image_size)),\n            transforms.ToTensor(),\n        ]\n        \n        if augment:\n            # Add augmentation for training\n            self.transform = transforms.Compose([\n                transforms.RandomHorizontalFlip(p=0.5),\n                transforms.RandomVerticalFlip(p=0.5),\n                transforms.RandomRotation(degrees=10),\n                *transform_list\n            ])\n        else:\n            self.transform = transforms.Compose(transform_list)\n    \n    def __len__(self):\n        return len(self.input_files)\n    \n    def __getitem__(self, idx):\n        # Load images\n        input_path = self.input_dir / self.input_files[idx]\n        target_path = self.target_dir / self.target_files[idx]\n        \n        input_img = Image.open(input_path).convert('RGB')\n        target_img = Image.open(target_path).convert('RGB')\n        \n        # Apply transforms\n        if self.augment:\n            # Apply same random transform to both images\n            seed = torch.randint(0, 2**32, (1,)).item()\n            torch.manual_seed(seed)\n            input_img = self.transform(input_img)\n            torch.manual_seed(seed)\n            target_img = self.transform(target_img)\n        else:\n            input_img = self.transform(input_img)\n            target_img = self.transform(target_img)\n        \n        return input_img, target_img"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create train and validation datasets using the loaded indices\ntrain_dataset = UnderwaterDataset(INPUT_DIR, TARGET_DIR, train_indices, image_size=256, augment=True)\nval_dataset = UnderwaterDataset(INPUT_DIR, TARGET_DIR, val_indices, image_size=256, augment=False)\n\nprint(f\"Training samples: {len(train_dataset)}\")\nprint(f\"Validation samples: {len(val_dataset)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3b. Create DataLoaders"
  },
  {
   "cell_type": "code",
   "source": "# Create optimized data loaders\nbatch_size = 16  # Adjust based on GPU memory\n\n# Optimize DataLoader settings for Colab\ntrain_loader = DataLoader(\n    train_dataset, \n    batch_size=batch_size, \n    shuffle=True, \n    num_workers=2,  # Colab typically has 2 CPU cores\n    pin_memory=True,  # Faster GPU transfer\n    prefetch_factor=2,  # Prefetch batches\n    persistent_workers=True  # Keep workers alive between epochs\n)\n\nval_loader = DataLoader(\n    val_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    num_workers=2,\n    pin_memory=True,\n    prefetch_factor=2,\n    persistent_workers=True\n)\n\nprint(f\"Batches per epoch: {len(train_loader)}\")\nprint(f\"DataLoader optimized with prefetching and persistent workers\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## 4. Define U-Net Based Autoencoder Architecture",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Double Convolution Block\"\"\"\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNetAutoencoder(nn.Module):\n",
    "    \"\"\"U-Net based autoencoder for image enhancement\"\"\"\n",
    "    def __init__(self, n_channels=3, n_classes=3):\n",
    "        super(UNetAutoencoder, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        # Encoder\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        self.down4 = Down(512, 1024)\n",
    "        \n",
    "        # Decoder\n",
    "        self.up1 = Up(1024, 512)\n",
    "        self.up2 = Up(512, 256)\n",
    "        self.up3 = Up(256, 128)\n",
    "        self.up4 = Up(128, 64)\n",
    "        self.outc = nn.Conv2d(64, n_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        \n",
    "        return torch.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = UNetAutoencoder(n_channels=3, n_classes=3).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define Loss Functions and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedLoss(nn.Module):\n",
    "    \"\"\"Combined loss function for image enhancement\"\"\"\n",
    "    def __init__(self, alpha=0.8, beta=0.2):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.l1_loss = nn.L1Loss()\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "        \n",
    "    def forward(self, pred, target):\n",
    "        l1 = self.l1_loss(pred, target)\n",
    "        mse = self.mse_loss(pred, target)\n",
    "        return self.alpha * l1 + self.beta * mse\n",
    "\n",
    "\n",
    "def calculate_psnr(img1, img2):\n",
    "    \"\"\"Calculate PSNR between two images\"\"\"\n",
    "    mse = torch.mean((img1 - img2) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    return 20 * torch.log10(1.0 / torch.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define loss function and optimizer\ncriterion = CombinedLoss(alpha=0.8, beta=0.2)\noptimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.999))\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n\n# Training configuration\nnum_epochs = 50\nbest_val_loss = float('inf')\npatience = 10\npatience_counter = 0"
  },
  {
   "cell_type": "code",
   "source": "def train_epoch(model, dataloader, criterion, optimizer, device):\n    \"\"\"Train for one epoch\"\"\"\n    model.train()\n    total_loss = 0.0\n    total_psnr = 0.0\n    num_batches = len(dataloader)\n    \n    with tqdm(dataloader, desc=\"Training\") as pbar:\n        for batch_idx, (inputs, targets) in enumerate(pbar):\n            inputs, targets = inputs.to(device), targets.to(device)\n            \n            # Zero gradients\n            optimizer.zero_grad()\n            \n            # Forward pass\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n            \n            # Backward pass\n            loss.backward()\n            optimizer.step()\n            \n            # Calculate metrics\n            batch_loss = loss.item()\n            batch_psnr = calculate_psnr(outputs, targets).item()\n            \n            total_loss += batch_loss\n            total_psnr += batch_psnr\n            \n            # Update progress bar\n            pbar.set_postfix({\n                'Loss': f'{batch_loss:.4f}',\n                'PSNR': f'{batch_psnr:.2f} dB'\n            })\n    \n    avg_loss = total_loss / num_batches\n    avg_psnr = total_psnr / num_batches\n    \n    return avg_loss, avg_psnr\n\n\ndef validate_epoch(model, dataloader, criterion, device):\n    \"\"\"Validate for one epoch\"\"\"\n    model.eval()\n    total_loss = 0.0\n    total_psnr = 0.0\n    num_batches = len(dataloader)\n    \n    with torch.no_grad():\n        with tqdm(dataloader, desc=\"Validation\") as pbar:\n            for batch_idx, (inputs, targets) in enumerate(pbar):\n                inputs, targets = inputs.to(device), targets.to(device)\n                \n                # Forward pass\n                outputs = model(inputs)\n                loss = criterion(outputs, targets)\n                \n                # Calculate metrics\n                batch_loss = loss.item()\n                batch_psnr = calculate_psnr(outputs, targets).item()\n                \n                total_loss += batch_loss\n                total_psnr += batch_psnr\n                \n                # Update progress bar\n                pbar.set_postfix({\n                    'Loss': f'{batch_loss:.4f}',\n                    'PSNR': f'{batch_psnr:.2f} dB'\n                })\n    \n    avg_loss = total_loss / num_batches\n    avg_psnr = total_psnr / num_batches\n    \n    return avg_loss, avg_psnr",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training history\ntrain_losses = []\nval_losses = []\ntrain_psnrs = []\nval_psnrs = []\n\n# Resume from checkpoint if exists\nCHECKPOINT_FILE = CHECKPOINT_PATH / 'latest_checkpoint.pth'\nstart_epoch = 0\n\nif CHECKPOINT_FILE.exists():\n    print(\"Found checkpoint, resuming training...\")\n    checkpoint = torch.load(CHECKPOINT_FILE)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n    start_epoch = checkpoint['epoch'] + 1\n    train_losses = checkpoint.get('train_losses', [])\n    val_losses = checkpoint.get('val_losses', [])\n    train_psnrs = checkpoint.get('train_psnrs', [])\n    val_psnrs = checkpoint.get('val_psnrs', [])\n    best_val_loss = checkpoint.get('best_val_loss', float('inf'))\n    print(f\"Resuming from epoch {start_epoch}\")\n\n# Training loop\nfor epoch in range(start_epoch, num_epochs):\n    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n    print(\"-\" * 50)\n    \n    # Train\n    train_loss, train_psnr = train_epoch(model, train_loader, criterion, optimizer, device)\n    train_losses.append(train_loss)\n    train_psnrs.append(train_psnr)\n    \n    # Validate\n    val_loss, val_psnr = validate_epoch(model, val_loader, criterion, device)\n    val_losses.append(val_loss)\n    val_psnrs.append(val_psnr)\n    \n    # Update learning rate\n    scheduler.step(val_loss)\n    \n    print(f\"Train Loss: {train_loss:.4f}, Train PSNR: {train_psnr:.2f} dB\")\n    print(f\"Val Loss: {val_loss:.4f}, Val PSNR: {val_psnr:.2f} dB\")\n    \n    # Save checkpoint to Google Drive\n    checkpoint = {\n        'epoch': epoch,\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'val_loss': val_loss,\n        'val_psnr': val_psnr,\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_psnrs': train_psnrs,\n        'val_psnrs': val_psnrs,\n        'best_val_loss': best_val_loss\n    }\n    \n    # Save latest checkpoint\n    torch.save(checkpoint, CHECKPOINT_FILE)\n    \n    # Save best model\n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        best_model_path = MODELS_PATH / 'best_underwater_enhancer.pth'\n        torch.save(checkpoint, best_model_path)\n        print(f\"‚úì Saved best model to Google Drive: {best_model_path}\")\n        patience_counter = 0\n    else:\n        patience_counter += 1\n    \n    # Save periodic checkpoint (every 5 epochs)\n    if (epoch + 1) % 5 == 0:\n        periodic_checkpoint = CHECKPOINT_PATH / f'checkpoint_epoch_{epoch+1}.pth'\n        torch.save(checkpoint, periodic_checkpoint)\n        print(f\"‚úì Saved periodic checkpoint to: {periodic_checkpoint}\")\n    \n    # Early stopping\n    if patience_counter >= patience:\n        print(f\"Early stopping triggered after {epoch+1} epochs\")\n        break\n\nprint(\"\\n‚úì Training complete!\")\nprint(f\"‚úì All models and checkpoints saved to Google Drive: {OUTPUT_BASE}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss plot\n",
    "ax1.plot(train_losses, label='Train Loss', color='blue')\n",
    "ax1.plot(val_losses, label='Val Loss', color='red')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training and Validation Loss')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# PSNR plot\n",
    "ax2.plot(train_psnrs, label='Train PSNR', color='blue')\n",
    "ax2.plot(val_psnrs, label='Val PSNR', color='red')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('PSNR (dB)')\n",
    "ax2.set_title('Training and Validation PSNR')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Load best model from Google Drive\nbest_model_path = MODELS_PATH / 'best_underwater_enhancer.pth'\n\nif best_model_path.exists():\n    checkpoint = torch.load(best_model_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"‚úì Loaded best model from Google Drive\")\n    print(f\"  Epoch: {checkpoint['epoch']+1}\")\n    print(f\"  Best Val Loss: {checkpoint['val_loss']:.4f}\")\n    print(f\"  Best Val PSNR: {checkpoint['val_psnr']:.2f} dB\")\nelse:\n    print(\"No saved model found in Google Drive\")\n    print(\"Please train the model first\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 8. Visualize Training History",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load best model from Google Drive\nbest_model_path = MODELS_PATH / 'best_underwater_enhancer.pth'\n\nif best_model_path.exists():\n    checkpoint = torch.load(best_model_path)\n    model.load_state_dict(checkpoint['model_state_dict'])\n    print(f\"‚úì Loaded best model from Google Drive\")\n    print(f\"  Epoch: {checkpoint['epoch']+1}\")\n    print(f\"  Best Val Loss: {checkpoint['val_loss']:.4f}\")\n    print(f\"  Best Val PSNR: {checkpoint['val_psnr']:.2f} dB\")\nelse:\n    print(\"No saved model found in Google Drive\")\n    print(\"Please train the model first\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export to ONNX for deployment (optional)\n",
    "dummy_input = torch.randn(1, 3, 256, 256).to(device)\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"underwater_enhancer.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    do_constant_folding=True,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'},\n",
    "                  'output': {0: 'batch_size'}}\n",
    ")\n",
    "print(\"Model exported to ONNX format as 'underwater_enhancer.onnx'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## 9. Load Best Model and Evaluate",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "def visualize_results(model, dataloader, device, num_samples=5):\n    model.eval()\n    \n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n    \n    with torch.no_grad():\n        for idx, (inputs, targets) in enumerate(dataloader):\n            if idx >= num_samples:\n                break\n            \n            inputs, targets = inputs.to(device), targets.to(device)\n            outputs = model(inputs)\n            \n            # Take first image from batch\n            input_img = inputs[0].cpu().permute(1, 2, 0).numpy()\n            target_img = targets[0].cpu().permute(1, 2, 0).numpy()\n            output_img = outputs[0].cpu().permute(1, 2, 0).numpy()\n            \n            # Display\n            axes[idx, 0].imshow(input_img)\n            axes[idx, 0].set_title('Input (Raw)')\n            axes[idx, 0].axis('off')\n            \n            axes[idx, 1].imshow(output_img)\n            axes[idx, 1].set_title('Model Output')\n            axes[idx, 1].axis('off')\n            \n            axes[idx, 2].imshow(target_img)\n            axes[idx, 2].set_title('Target (Manual Edit)')\n            axes[idx, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n# Visualize results\nvisualize_results(model, val_loader, device, num_samples=5)",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": "# Save final model to Google Drive\nfinal_model_path = MODELS_PATH / 'underwater_enhancer_final.pth'\n\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'model_config': {\n        'n_channels': 3,\n        'n_classes': 3,\n        'image_size': 256\n    },\n    'training_history': {\n        'train_losses': train_losses,\n        'val_losses': val_losses,\n        'train_psnrs': train_psnrs,\n        'val_psnrs': val_psnrs\n    }\n}, final_model_path)\n\nprint(f\"‚úì Model saved to Google Drive: {final_model_path}\")"
  },
  {
   "cell_type": "markdown",
   "source": "## 10. Save Final Models",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model files from Colab\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading model files...\")\n",
    "files.download('best_underwater_enhancer.pth')\n",
    "files.download('underwater_enhancer_final.pth')\n",
    "files.download('underwater_enhancer.onnx')\n",
    "print(\"Download complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}